{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33930fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models_used': ['meta-llama/Llama-3. 1-70B-Instruct'],\n",
       " 'rounds': 2,\n",
       " 'number_of_agents': 3,\n",
       " 'problem': 'You are given the following symptoms joint_pain, skin_peeling, silver_like_dusting, small_dents_in_nails, inflammatory_nails. Your task is to diagnose a disease that best matches the symptoms.',\n",
       " 'gold_diagnosis': 'Psoriasis',\n",
       " 'conversation_id': 803,\n",
       " 'history_current': \"Both responses accurately diagnose the condition as Psoriasis based on the provided symptoms. However, Response 2 provides a clearer explanation of each symptom's relevance to Psoriasis, making it a more comprehensive and informative answer. A definitive diagnosis by a dermatologist is still necessary for confirmation.\",\n",
       " 'history_neighbour': 'Both responses suggest psoriasis as the likely diagnosis, and they are consistent in their descriptions of the characteristic symptoms. However, Response 1 provides a clearer explanation of the underlying cause of psoriasis, describing it as an autoimmune condition that causes inflammation and skin cell buildup. Response 2 is more concise but lacks this explanatory detail. An improved response would combine the clarity of Response 1 with the concision of Response 2.',\n",
       " 'reply': 'Based on the symptoms provided, the likely diagnosis is Psoriasis. This autoimmune condition leads to inflammation and an abnormal buildup of skin cells, causing symptoms such as joint pain, skin peeling, and silver-like dusting. The presence of small dents in nails and inflammatory nails further supports this diagnosis. However, a definitive diagnosis by a dermatologist is necessary for confirmation. This response combines the clarity of Response 1 with the comprehensiveness of Response 2, providing a concise and'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "my_dataset = load_from_disk(\"/home/UNT/ap1923/ap1923/betterDeliberation/src/deliberation/storedDeliberations/context110/new110/my_hf_dataset_new110\")\n",
    "my_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574a9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['models_used', 'rounds', 'number_of_agents', 'problem', 'gold_diagnosis', 'conversation_id', 'history_current', 'history_neighbour', 'reply'],\n",
       "        num_rows: 5827\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['models_used', 'rounds', 'number_of_agents', 'problem', 'gold_diagnosis', 'conversation_id', 'history_current', 'history_neighbour', 'reply'],\n",
       "        num_rows: 728\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['models_used', 'rounds', 'number_of_agents', 'problem', 'gold_diagnosis', 'conversation_id', 'history_current', 'history_neighbour', 'reply'],\n",
       "        num_rows: 729\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0d2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# meta-llama/Llama-3.1-8B-Instruct\n",
    "# meta-llama/Llama-3.2-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e712d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    combined_inputs = [\n",
    "        f\"{p} {hc} {hn} {r}\"\n",
    "        for p, hc, hn, r in zip(\n",
    "            examples[\"problem\"],\n",
    "            examples[\"history_current\"],\n",
    "            examples[\"history_neighbour\"],\n",
    "            examples[\"reply\"],\n",
    "        )\n",
    "    ]\n",
    "    return tokenizer(combined_inputs, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa40110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21280db693fb4eb393e85cdaa977607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5827 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03709812e2d24b5280daf7567a07b40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4801d76e284c2a8b7bbec71cc2e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = my_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns= my_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c19bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02ac934063b4dc59e88c8cbbcb9d8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5827 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3575ba143c844138b74bb2af0402aafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53298b5328a446d48a9123220a54b9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_data.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0354df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d88cb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model_untouched = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de7ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422631/822812716.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17481' max='17481' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17481/17481 10:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.163400</td>\n",
       "      <td>1.074373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.021000</td>\n",
       "      <td>0.973795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>0.946868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('finetunedDistilledBert_model/tokenizer_config.json',\n",
       " 'finetunedDistilledBert_model/special_tokens_map.json',\n",
       " 'finetunedDistilledBert_model/vocab.json',\n",
       " 'finetunedDistilledBert_model/merges.txt',\n",
       " 'finetunedDistilledBert_model/added_tokens.json',\n",
       " 'finetunedDistilledBert_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetunedDistilledBert_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"finetunedDistilledBert_model\")\n",
    "tokenizer.save_pretrained(\"finetunedDistilledBert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd8d8768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='729' max='729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [729/729 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.58\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b4e1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_422631/3312912539.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  untouched_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='729' max='729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [729/729 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 31.40\n"
     ]
    }
   ],
   "source": [
    "untouched_trainer = Trainer(\n",
    "    model=model_untouched,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "eval_results = untouched_trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f57b4c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1 result [{'generated_text': 'What will be the most likely disease diagnosis for a patient showing the following symptoms: continous sneezing, chills and watering of the eye. Impetigo is a highly contagious bacterial skin infection that typically causes red sores, black sores'}]\n",
      "prompt2 result [{'generated_text': 'You are given the following symptoms: continous sneezing, chills and watering of the eye. Your task is to diagnose a disease that best matches the symptoms. Both responses accurately diagnose the condition as Allergic Rhinitis based on the provided'}]\n",
      "prompt1 result from untouched model [{'generated_text': 'What will be the most likely disease diagnosis for a patient showing the following symptoms: continous sneezing, chills and watering of the eye, cough, and diarrhea.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n",
      "prompt2 result from untouched model [{'generated_text': 'You are given the following symptoms: continous sneezing, chills and watering of the eye. Your task is to diagnose a disease that best matches the symptoms. Your condition is a chronic condition with a high level of infection or infection.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What will be the most likely disease diagnosis for a patient showing the following symptoms: continous sneezing, chills and watering of the eye\"\n",
    "# using conversation_id 11 and the gold diagnosis is allergy\n",
    "prompt_two = \"You are given the following symptoms: continous sneezing, chills and watering of the eye. Your task is to diagnose a disease that best matches the symptoms.\"\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"finetunedDistilledBert_model\")\n",
    "print('prompt1 result', generator(prompt))\n",
    "print('prompt2 result', generator(prompt_two))\n",
    "\n",
    "generator_two = pipeline(\"text-generation\", model=model_untouched, tokenizer=tokenizer)\n",
    "print('prompt1 result from untouched model', generator_two(prompt))\n",
    "print('prompt2 result from untouched model', generator_two(prompt_two))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcb4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine tuned model result ['What will be the most likely disease diagnosis for a patient showing the following symptoms: continous sneezing, chills and watering of the eye A medical professional would diagnose the condition as Influenza (Flu). The combination of symptoms such as continuous sneezing, chills, watering from the eyes, and watering from the eyes suggest an upper respiratory tract infection. It is essential to consult a healthcare professional for a definitive diagnosis and appropriate treatment. Based on the symptoms provided, the disease diagnosis is likely Influenza (Flu). The combination of symptoms such as continuous sneezing, chills, and watering of the eyes strongly suggests Influ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What will be the most likely disease diagnosis for a patient showing the following symptoms: continous sneezing, chills and watering of the eye, mouth, nose, eyes, back, chest, and chest â€” those who have a high risk of developing an infection or a disease such as flu, flu, or a few more types of pneumonia.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_path = \"/home/UNT/ap1923/ap1923/modelfinetuning/finetunedDistilledBert_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_three = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "device = model_three.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model_three.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print('fine tuned model result', tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "# same thing but from untouched model\n",
    "device = model_untouched.device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model_untouched.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eddfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
