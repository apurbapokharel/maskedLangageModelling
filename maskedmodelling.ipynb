{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3db6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d80324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'answers.text_urls', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'answers.text_urls', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "from datasets import load_from_disk\n",
    "eli5 = load_from_disk(\"~/ap1923/eli5_5k_split\")\n",
    "eli5 = eli5.flatten()\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f10c78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '798v9m',\n",
       " 'title': 'What would happen if one of my eyes are covered for a long time?',\n",
       " 'selftext': '',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['dp03dep', 'dp031p3'],\n",
       " 'answers.text': [\"If you are a child and still in a developing stage for vision and perception, covering one of your eyes can cause amblyopia. Amblyopia -a disorder also called lazy eye- causes decreased vision in covered eye due to the interruption in the eye-brain pathway. If done with growing up, i think it is fine to strut around with pirate's eyepatch.\",\n",
       "  'What do you mean by \"a long time\"? Is it half an hour? Half a month? Five years? Twenty years? In like 15 minutes, your pupil would dilate(non-native English speaker, is this right? I am thinking expand), letting in more light. Therefore, if you plan on going somewhere dark, or you think it suddenly will become dark where you are, wear an eyepatch on eye. When entering the dark room, switch Wich eye you are covering. Instant night vision. If you are talking about covering an eye for more than a day, better find someone else than me.'],\n",
       " 'answers.score': [5, 3],\n",
       " 'answers.text_urls': [[], []],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9637d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'answers.text_urls', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'answers.text_urls', 'title_urls', 'selftext_urls'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"length\", eli5[\"train\"][0][\"answers.text\"])\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7074517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60edde46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_eli5)\n",
    "# print(\"length\", len(tokenized_eli5[\"train\"][0]['input_ids']))\n",
    "# tokenized_eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2125fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    print(\"examples\", examples)\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe07bfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 10719\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2441\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37dde14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4fdd8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3302686/1942845635.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4020' max='4020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4020/4020 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.711200</td>\n",
       "      <td>2.116071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.880300</td>\n",
       "      <td>2.086677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.083100</td>\n",
       "      <td>2.049644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('my_awesome_eli5_mlm_model/tokenizer_config.json',\n",
       " 'my_awesome_eli5_mlm_model/special_tokens_map.json',\n",
       " 'my_awesome_eli5_mlm_model/vocab.json',\n",
       " 'my_awesome_eli5_mlm_model/merges.txt',\n",
       " 'my_awesome_eli5_mlm_model/added_tokens.json',\n",
       " 'my_awesome_eli5_mlm_model/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_mlm_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"my_awesome_eli5_mlm_model\")\n",
    "tokenizer.save_pretrained(\"my_awesome_eli5_mlm_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f19642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/306 00:00 < 00:02, 108.30 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7.69\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb30ba1",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d71053ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6642128825187683,\n",
       "  'token': 21300,\n",
       "  'token_str': ' spiral',\n",
       "  'sequence': 'The Milky Way is a spiral galaxy.'},\n",
       " {'score': 0.06015821918845177,\n",
       "  'token': 2232,\n",
       "  'token_str': ' massive',\n",
       "  'sequence': 'The Milky Way is a massive galaxy.'},\n",
       " {'score': 0.03243763744831085,\n",
       "  'token': 3065,\n",
       "  'token_str': ' giant',\n",
       "  'sequence': 'The Milky Way is a giant galaxy.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The Milky Way is a <mask> galaxy.\"\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"my_awesome_eli5_mlm_model\",\n",
    "    tokenizer=\"my_awesome_eli5_mlm_model\"\n",
    ")\n",
    "mask_filler(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58405012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_logits torch.Size([1, 50265]) tensor([[-2.5637, -3.2536,  4.1820,  ..., -1.9573, -2.9342,  2.7244]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "model_path = \"/home/UNT/ap1923/ap1923/maskedmodelling/my_awesome_eli5_mlm_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "text = \"The Milky Way is a <mask> galaxy.\"\n",
    "# text2 = \"The Milky Way is a galaxy.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# inputs2 = tokenizer(text2, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "# print('inputs', inputs)\n",
    "# print('inputs', inputs2)\n",
    "# print('decoded', tokenizer.decode([50264]))\n",
    "# print('mask token index', mask_token_index)\n",
    "# print('mask token id', tokenizer.mask_token_id)\n",
    "# print('mask token index', mask_token_index)\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "print('mask_token_logits', mask_token_logits.shape,  mask_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfcfa1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Milky Way is a  spiral galaxy.\n",
      "The Milky Way is a  massive galaxy.\n",
      "The Milky Way is a  giant galaxy.\n"
     ]
    }
   ],
   "source": [
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n",
    "# The Milky Way is a spiral galaxy.\n",
    "# The Milky Way is a massive galaxy.\n",
    "# The Milky Way is a small galaxy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
